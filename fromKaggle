import tensorflow as tf
from kaggle_datasets import KaggleDatasets

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    strategy = tf.distribute.get_strategy()
print("REPLICAS: ", strategy.num_replicas_in_sync)


BATCH_SIZE = 16 * strategy.num_replicas_in_sync
print(BATCH_SIZE)
GCS_DS_PATH = KaggleDatasets().get_gcs_path()
print(GCS_DS_PATH)
EPOCHS = 30

import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt
import numpy as np
categories = ['00','01','02','03','04','05','06','07','08','09','10','11','12',
              '13','14','15','16','17','18','19','20','21','22','23','24','25',
              '26','27','28','29','30','31','32','33','34','35','36','37','38',
              '39','40','41']
train_resource = os.path.join(GCS_DS_PATH,'train','train','train')
test_resource = os.path.join(GCS_DS_PATH,'test','test','test')
print(train_resource)


traindf = pd.read_csv(r'../input/shopee-product-detection-open/train.csv')
testdf = pd.read_csv(r'../input/shopee-product-detection-open/test.csv')
print(traindf.head())
print(testdf.head())


tranfile = {}
cate = [i for i in range(42)]
for cat in cate:
    try:
        tranfile[cat] = traindf['filename'][traindf['category'] == cat].sample(2500)
    except:
        tranfile[cat] = traindf['filename'][traindf['category'] == cat].sample(frac = 1.)
print(tranfile.keys())
print(len(tranfile))



train_paths = []
train_labels = []
for i, ele in enumerate(tranfile.keys()):
    if i == ele:
        for img in tranfile[i]:
            train_paths.append(os.path.join(train_resource, categories[ele], img))
        print(len(train_paths))
for i in tranfile.keys():
    train_labels.extend([i]*len(tranfile[i]))
print(len(train_paths))
print(len(train_labels))   
print(train_paths,train_labels)




from tensorflow.keras.utils import to_categorical
train_paths = np.array(train_paths)
train_labels = to_categorical(train_labels) 
print(train_paths,train_labels)



from sklearn.model_selection import train_test_split
train_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, 
                                                                        train_labels, 
                                                                        stratify=train_labels,
                                                                        test_size=0.1, 
                                                                        random_state=2020)
train_paths.shape, valid_paths.shape, train_labels.shape, valid_labels.shape



test_paths = []
for img in testdf['filename']:
    test_paths.append(os.path.join(GCS_DS_PATH,  'test', 'test', 'test', img))
test_paths = np.array(test_paths)
print(test_paths)



def decode_image(filename, label=None):
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits, channels=3)
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, (400,400))   
    if label is None:
        return image
    else:
        return image, label
    
    
    
def data_augment(image, label=None):
    image = tf.image.random_flip_left_right(image)    
    if label is None:
        return image
    else:
        return image, label
    
    
train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((train_paths, train_labels))
    .map(decode_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    .map(data_augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    .cache()
    .repeat()
    .shuffle(2048)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.experimental.AUTOTUNE)
)

valid_dataset = (
    tf.data.Dataset
    .from_tensor_slices((valid_paths, valid_labels))
    .map(decode_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    .batch(BATCH_SIZE)
    .cache()
    .prefetch(tf.data.experimental.AUTOTUNE)
)

test_dataset = (
    tf.data.Dataset
    .from_tensor_slices(test_paths)
    .map(decode_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    .batch(BATCH_SIZE)
)



!pip install -q efficientnet



from tensorflow.keras.layers import Dense
from efficientnet.tfkeras import EfficientNetB7
import keras.backend as K



with strategy.scope():
    model = tf.keras.Sequential([
        EfficientNetB7(weights='noisy-student',
                       include_top=False,
                       pooling='avg'), 
        Dense(42, activation='softmax')
    ])
    
    model.layers[0].trainable = False
    
    model.compile(optimizer = 'adam',
                  loss = "categorical_crossentropy",
                  metrics=['accuracy'])
    
    model.summary()
    
    
    
n_steps = train_labels.shape[0] // BATCH_SIZE  # 88057 / 128 = 687
print(train_labels.shape[0])
print(n_steps)
history = model.fit(
    train_dataset, 
    steps_per_epoch=n_steps,
    validation_data=valid_dataset,
    epochs=EPOCHS,
)



prediction = model.predict(test_dataset, verbose=1)



testdf = testdf.drop('category', axis=1)
testdf['category'] = prediction.argmax(axis=1)
testdf['category'] = testdf['category'].apply(lambda x: str(x).zfill(2))
print(testdf)
testdf.to_csv('sub.csv', index=False)

